"""A clean version of efficient moba implementation with flash-attn"""

import torch

from flash_attn import flash_attn_varlen_func
from flash_attn.flash_attn_interface import (
    _flash_attn_varlen_forward,
    _flash_attn_varlen_backward,
)
from functools import lru_cache
from einops import rearrange


@lru_cache(maxsize=16)
def calc_chunks(cu_seqlen, moba_chunk_size):
    """calc chunks that needs moba attention"""

    # batch_sizes[batch_idx] = batch size ( seqlen ) of batch idx
    batch_sizes = cu_seqlen[1:] - cu_seqlen[:-1]
    # batch_num_chunk[batch_idx] = how many chunk in batch idx
    batch_num_chunk = (batch_sizes + (moba_chunk_size - 1)) // moba_chunk_size
    # cu_num_chunk[batch_idx] = first chunk id of this batch
    cu_num_chunk = torch.ones(
        batch_num_chunk.numel() + 1,
        device=cu_seqlen.device,
        dtype=cu_seqlen.dtype,
    )
    cu_num_chunk[1:] = batch_num_chunk.cumsum(dim=0)
    # total chunk ( for all batch )
    num_chunk = cu_num_chunk[-1]
    # chunk_sizes[chunk_idx] = chunk_size of chunk idx
    chunk_sizes = torch.full(
        (num_chunk + 1,), moba_chunk_size, dtype=cu_seqlen.dtype, device=cu_seqlen.device
    )
    chunk_sizes[0] = 0  # for calc cu chunk
    batch_last_chunk_size = batch_sizes - (batch_num_chunk - 1) * moba_chunk_size
    print(batch_last_chunk_size)
    print(cu_num_chunk)
    print(chunk_sizes)
    chunk_sizes[cu_num_chunk[1:]] = batch_last_chunk_size
    # cu_chunk[chunk_idx] = the start chunk offset of chunk idx
    cu_chunk = chunk_sizes.cumsum(dim=-1, dtype=torch.int32)
    # chunk_to_batch[chunk_idx] = batch idx of the chunk idx
    chunk_to_batch = torch.zeros(
        (num_chunk,), dtype=torch.int32, device=cu_seqlen.device
    )
    chunk_to_batch[cu_num_chunk[1:-1]] = 1
    chunk_to_batch = chunk_to_batch.cumsum(dim=0, dtype=torch.int32)

    """ filter chunks that need moba attn """

    # filter chunks ( remove last chunk of each batch )
    # filtered_chunk_indices: chunk index list that excludes the last chunk of each batch
    chunk_to_remove = cu_num_chunk[1:] - 1
    chunk_to_remain = torch.ones(
        (num_chunk,), dtype=torch.bool, device=cu_seqlen.device
    )
    chunk_to_remain[chunk_to_remove] = False
    filtered_chunk_indices = chunk_to_remain.nonzero(as_tuple=True)[0]
    num_filtered_chunk = len(filtered_chunk_indices)

    return (
        cu_chunk,
        filtered_chunk_indices,
        num_filtered_chunk,
        chunk_to_batch,
    )


class MixedAttention(torch.autograd.Function):

    @staticmethod
    def forward(
        ctx,
        q,
        k,
        v,
        self_attn_cu_seqlen,
        moba_q,
        moba_kv,
        moba_cu_seqlen_q,
        moba_cu_seqlen_kv,
        max_seqlen,
        moba_chunk_size,
        moba_q_sh_indices,
    ):
        ctx.max_seqlen = max_seqlen
        ctx.moba_chunk_size = moba_chunk_size
        ctx.softmax_scale = softmax_scale = q.shape[-1] ** (-0.5)

        # self attn
        self_attn_out_sh, self_attn_lse_hs, _, _ = (
            _flash_attn_varlen_forward(
                q=q,
                k=k,
                v=v,
                cu_seqlens_q=self_attn_cu_seqlen,
                cu_seqlens_k=self_attn_cu_seqlen,
                max_seqlen_q=max_seqlen,
                max_seqlen_k=max_seqlen,
                softmax_scale=softmax_scale,
                causal=True,
                dropout_p=0.0,
            )
        )

        # moba attn
        moba_attn_out, moba_attn_lse_hs, _, _ = _flash_attn_varlen_forward(
            q=moba_q,
            k=moba_kv[:, 0],
            v=moba_kv[:, 1],
            cu_seqlens_q=moba_cu_seqlen_q,
            cu_seqlens_k=moba_cu_seqlen_kv,
            max_seqlen_q=max_seqlen,
            max_seqlen_k=moba_chunk_size,
            softmax_scale=softmax_scale,
            causal=False,
            dropout_p=0.0,
        )

        # convert lse shape hs -> sh ( follow the legacy mix attn logic )
        self_attn_lse_sh = self_attn_lse_hs.t().contiguous()
        moba_attn_lse = moba_attn_lse_hs.t().contiguous()

        # output buffer [S, H, D], same shape as q
        output = torch.zeros(
            (q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32
        )

        # flatten vS & H for index ops
        output_2d = output.view(-1, q.shape[2])

        # calc mixed_lse
        # minus max lse to avoid exp explosion
        max_lse_1d = self_attn_lse_sh.view(-1)
        max_lse_1d = max_lse_1d.index_reduce(
            0, moba_q_sh_indices, moba_attn_lse.view(-1), "amax"
        )
        self_attn_lse_sh = self_attn_lse_sh - max_lse_1d.view_as(self_attn_lse_sh)
        moba_attn_lse = (
            moba_attn_lse.view(-1)
            .sub(max_lse_1d.index_select(0, moba_q_sh_indices))
            .reshape_as(moba_attn_lse)
        )

        mixed_attn_se_sh = self_attn_lse_sh.exp()
        moba_attn_se = moba_attn_lse.exp()

        mixed_attn_se_sh.view(-1).index_add_(
            0, moba_q_sh_indices, moba_attn_se.view(-1)
        )
        mixed_attn_lse_sh = mixed_attn_se_sh.log()

        # add attn output
        factor = (self_attn_lse_sh - mixed_attn_lse_sh).exp()  # [ vS, H ]
        self_attn_out_sh = self_attn_out_sh * factor.unsqueeze(-1)
        output_2d += self_attn_out_sh.reshape_as(output_2d)

        # add moba output
        mixed_attn_lse = (
            mixed_attn_lse_sh.view(-1)
            .index_select(0, moba_q_sh_indices)
            .view_as(moba_attn_lse)
        )
        factor = (moba_attn_lse - mixed_attn_lse).exp()  # [ vS, H ]
        moba_attn_out = moba_attn_out * factor.unsqueeze(-1)
        raw_attn_out = moba_attn_out.view(-1, moba_attn_out.shape[-1])
        output_2d.index_add_(0, moba_q_sh_indices, raw_attn_out)
        output = output.to(q.dtype)
        # add back max lse
        mixed_attn_lse_sh = mixed_attn_lse_sh + max_lse_1d.view_as(mixed_attn_se_sh)
        ctx.save_for_backward(
            output,
            mixed_attn_lse_sh,
            q,
            k,
            v,
            self_attn_cu_seqlen,
            moba_q,
            moba_kv,
            moba_cu_seqlen_q,
            moba_cu_seqlen_kv,
            moba_q_sh_indices,
        )

        return output

    @staticmethod
    def backward(ctx, d_output):

        max_seqlen = ctx.max_seqlen
        moba_chunk_size = ctx.moba_chunk_size
        softmax_scale = ctx.softmax_scale

        (
            output,
            mixed_attn_vlse_sh,
            q,
            k,
            v,
            self_attn_cu_seqlen,
            moba_q,
            moba_kv,
            moba_cu_seqlen_q,
            moba_cu_seqlen_kv,
            moba_q_sh_indices,
        ) = ctx.saved_tensors

        d_output = d_output.contiguous()

        dq = torch.zeros_like(q)
        dk = torch.zeros_like(k)
        dv = torch.zeros_like(v)
        _ = _flash_attn_varlen_backward(
            dout=d_output,
            q=q,
            k=k,
            v=v,
            out=output,
            softmax_lse=mixed_attn_vlse_sh.t().contiguous(),
            dq=dq,
            dk=dk,
            dv=dv,
            cu_seqlens_q=self_attn_cu_seqlen,
            cu_seqlens_k=self_attn_cu_seqlen,
            max_seqlen_q=max_seqlen,
            max_seqlen_k=max_seqlen,
            softmax_scale=softmax_scale,
            causal=True,
            dropout_p=0.0,
            window_size_left=-1,
            window_size_right=-1,
            softcap=0.0,
            alibi_slopes=None,
            deterministic=True,
        )

        headdim = q.shape[-1]
        d_moba_output = (
            d_output.view(-1, headdim).index_select(0, moba_q_sh_indices).unsqueeze(1)
        )
        moba_output = (
            output.view(-1, headdim).index_select(0, moba_q_sh_indices).unsqueeze(1)
        )

        mixed_attn_vlse = (
            mixed_attn_vlse_sh.view(-1).index_select(0, moba_q_sh_indices).view(1, -1)
        )

        dmq = torch.zeros_like(moba_q)
        dmk = torch.zeros_like(moba_kv[:, 0])
        dmv = torch.zeros_like(moba_kv[:, 1])
        _ = _flash_attn_varlen_backward(
            dout=d_moba_output,
            q=moba_q,
            k=moba_kv[:, 0],
            v=moba_kv[:, 1],
            out=moba_output,
            softmax_lse=mixed_attn_vlse,
            dq=dmq,
            dk=dmk,
            dv=dmv,
            cu_seqlens_q=moba_cu_seqlen_q,
            cu_seqlens_k=moba_cu_seqlen_kv,
            max_seqlen_q=max_seqlen,
            max_seqlen_k=moba_chunk_size,
            softmax_scale=softmax_scale,
            causal=False,
            dropout_p=0.0,
            window_size_left=-1,
            window_size_right=-1,
            softcap=0.0,
            alibi_slopes=None,
            deterministic=True,
        )

        dmkv = torch.stack((dmk, dmv), dim=1)
        return dq, dk, dv, None, dmq, dmkv, None, None, None, None, None


def moba_attn_varlen(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    cu_seqlens: torch.Tensor,
    max_seqlen: int,
    moba_chunk_size: int,
    moba_topk: int,
) -> torch.Tensor:
    """An efficient version of moba implementation with triton kernels and flash-attn, the core logic:
    1. Calculate the chunks and the number of chunks, n = floor(data_size / chunk_size)
       - tokens in the tail chunk are reserved for self attn
       - tokens in other chunks will be processed in later steps
    2. K in each chunk will calculate mean value as the representative k, and Q will attend to these representative
    k to get the gate logit, which will be used to select topk chunks
    3. Select the topk chunks and get the dense q for each kv chunk pair and do the varlen attention
    4. Combine the varlen attn and self attn results via online softmax to get the final result

    Args:
        q (torch.Tensor): [seqlen, head, head_dim]
        k (torch.Tensor): [seqlen, head, head_dim]
        v (torch.Tensor): [seqlen, head, head_dim]
        cu_seqlens (torch.Tensor): the cumulative sequence length tensor, same definition in flash attn
        max_seqlen (int): the max sequence length of the batch, same definition in flash attn

    Returns:
        attn_output (torch.Tensor): [seqlen, head, head_dim]
    """

    kv = torch.stack((k, v), dim=1)

    """ some basic variables """
    # qkv shape = [ S, H, D ]
    seqlen, num_head, head_dim = q.shape

    """ prepare chunk meta """
    (
        cu_chunk,
        filtered_chunk_indices,
        num_filtered_chunk,
        chunk_to_batch,
    ) = calc_chunks(cu_seqlens, moba_chunk_size)

    # we will adjust selective topk to moba_topk - 1, as the last chunk is always chosen
    moba_topk = min(moba_topk - 1, num_filtered_chunk)
    need_moba_attn = moba_topk > 0

    # corner case: if no moba attn needed, just return self attn
    if not need_moba_attn:
        return flash_attn_varlen_func(
            q, k, v, cu_seqlens, cu_seqlens, max_seqlen, max_seqlen, causal=True
        )

    self_attn_cu_seqlen = cu_chunk

    # filtered_kv is a dense matrix that only contains filtered chunk of kv
    filtered_kv_indices = torch.arange(
        0, moba_chunk_size, dtype=torch.int32, device=q.device
    )[None, :].repeat(num_filtered_chunk, 1)
    filtered_kv_indices += cu_chunk[filtered_chunk_indices][:, None]
    filtered_kv = kv.index_select(0, filtered_kv_indices.view(-1))

    """ calc key_gate_weight and gate """

    # key_gate_weight [ F_N_CHUNK, HEAD, HEAD_DIM ]
    key_gate_weight = (
        filtered_kv[:, 0]
        .view(num_filtered_chunk, moba_chunk_size, num_head, head_dim)
        .mean(dim=1)
        .float()
    )
    q = q.type(torch.float32)  # float logit on the fly for better gate logit perception
    key_gate_weight = key_gate_weight.type(
        torch.float32
    )  # float logit for better gate logit perception
    gate = torch.einsum(
        "nhd,shd->nhs", key_gate_weight, q
    )  # gate [ F_N_CHUNK, HEAD, SEQ ]
    key_gate_weight = key_gate_weight.type_as(k)
    q = q.type_as(k)

    # pose process gate, masking unchosen batch and apply causal mask to current chunk
    gate_seq_idx = torch.arange(0, seqlen, device=q.device, dtype=torch.int32)[
        None, :
    ].repeat(num_filtered_chunk, 1)
    chunk_end = cu_chunk[filtered_chunk_indices + 1]
    batch_end = cu_seqlens[chunk_to_batch[filtered_chunk_indices] + 1]
    gate_chunk_end_mask = gate_seq_idx < chunk_end[:, None]
    gate_batch_end_mask = gate_seq_idx >= batch_end[:, None]
    gate_inf_mask = gate_chunk_end_mask | gate_batch_end_mask
    gate.masked_fill_(gate_inf_mask.unsqueeze(1), -float("inf"))

    """ find moba q that needs moba attn """
    # find topk chunks
    # gate_mask [ N_CHUNK, HEAD, SEQ ], true indicates that needs attention
    _, gate_top_k_idx = torch.topk(gate, k=moba_topk, dim=0, largest=True, sorted=False)
    # apply causal mask
    gate_mask = torch.logical_not(gate.isinf())
    # select topk chunks
    gate_idx_mask = torch.zeros(gate_mask.shape, dtype=torch.bool, device=q.device)
    gate_idx_mask = gate_idx_mask.scatter_(dim=0, index=gate_top_k_idx, value=True)
    gate_mask = torch.logical_and(gate_mask, gate_idx_mask)

    # varlen trick: combining all q index that needs moba attn
    # the result will be like [ C0H0 ][ C0H1 ][ C0H2 ][ ... ][ CnHm ]
    moba_q_indices = gate_mask.reshape(gate_mask.shape[0], -1).nonzero(as_tuple=True)[
        -1
    ]  # [ HS indices ] * N
    # moba_seqlen_q indicates that how many q chunks are selected for each kv chunk - head
    moba_seqlen_q = gate_mask.sum(dim=-1).flatten()
    # select all q that needs moba attn based on the moba_q_indices
    moba_q = rearrange(q, "s h d -> ( h s ) d").index_select(
        0, moba_q_indices
    )  # [ selected_S, D ]
    moba_q = moba_q.unsqueeze(1)
    # moba_q_sh_indices represents the position in the origin q tensor of each q token inside moba_q
    moba_q_sh_indices = moba_q_indices % seqlen * num_head + moba_q_indices // seqlen

    """ prepare moba kv """
    # Since moba_q is organized as HS * N, we need to reorganize kv to adapt to q

    # cut off zero experts
    q_zero_mask = moba_seqlen_q == 0
    valid_expert_mask = ~q_zero_mask
    zero_expert_count = q_zero_mask.sum()
    # only keep the kv that has q select > 0
    if zero_expert_count > 0:
        moba_seqlen_q = moba_seqlen_q[valid_expert_mask]
    # moba cu_seqlen for flash attn
    moba_cu_seqlen_q = torch.cat(
        (
            torch.tensor([0], device=q.device, dtype=moba_seqlen_q.dtype),
            moba_seqlen_q.cumsum(dim=0),
        ),
        dim=0,
    ).to(torch.int32)
    moba_kv = rearrange(filtered_kv, "s x h d -> h s x d")
    moba_kv = moba_kv.split(moba_chunk_size, dim=1)
    moba_kv = torch.cat(moba_kv, dim=0)
    if zero_expert_count > 0:
        assert valid_expert_mask.sum() == moba_kv.shape[0] - zero_expert_count
        moba_kv = moba_kv[
            valid_expert_mask
        ]  # cut off zero Q expert from kv , or the grad may be nan
    moba_kv = moba_kv.flatten(start_dim=0, end_dim=1).unsqueeze(2)
    moba_cu_seqlen_kv = (
        torch.arange(
            0,
            num_filtered_chunk * num_head + 1 - zero_expert_count,
            dtype=torch.int32,
            device=q.device,
        )
        * moba_chunk_size
    )

    # Shape check
    assert (
        moba_cu_seqlen_kv.shape == moba_cu_seqlen_q.shape
    ), f"moba_cu_seqlen_kv.shape != moba_cu_seqlen_q.shape {moba_cu_seqlen_kv.shape} != {moba_cu_seqlen_q.shape}"

    # Wrapping up the flash attn call and online softmax dlse inside MixedAttention class
    return MixedAttention.apply(
        q,
        k,
        v,
        self_attn_cu_seqlen,
        moba_q,
        moba_kv,
        moba_cu_seqlen_q,
        moba_cu_seqlen_kv,
        max_seqlen,
        moba_chunk_size,
        moba_q_sh_indices,
    )
if __name__ == "__main__":
    # --- å‚æ•°é…ç½® ---
    # æ¨¡æ‹Ÿä¸€ä¸ªåŒ…å«3ä¸ªåºåˆ—çš„æ‰¹æ¬¡
    seqlens = [512, 1024, 256] 
    batch_size = len(seqlens)
    total_seqlen = sum(seqlens)
    max_seqlen = max(seqlens)
    num_heads = 8
    head_dim = 64
    # MoBA ç‰¹å®šå‚æ•°
    moba_chunk_size = 256
    moba_topk = 2  # é€‰æ‹© top-1 å†å² chunk + å½“å‰ chunk
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨ï¼Œå¦‚æœä¸å¯ç”¨åˆ™ç»™å‡ºæç¤º
    if device == "cpu":
        print("è­¦å‘Šï¼šCUDA ä¸å¯ç”¨ï¼Œå°†åœ¨ CPU ä¸Šè¿è¡Œã€‚FlashAttention å’Œ Triton å†…æ ¸éœ€è¦ CUDAã€‚")
        print("æµ‹è¯•å°†ä»…éªŒè¯ä»£ç é€»è¾‘å’Œå½¢çŠ¶ï¼Œæ€§èƒ½å’Œæ•°å€¼ç»“æœå¯èƒ½ä¸å‡†ç¡®ã€‚")
        dtype = torch.float32 # CPU å¯¹ float16 æ”¯æŒä¸ä½³
    print("--- æµ‹è¯•ç¯å¢ƒé…ç½® ---")
    print(f"æ—¥æœŸ: 2025/11/13")
    print(f"è®¾å¤‡: {device}")
    print(f"æ•°æ®ç±»å‹: {dtype}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"åºåˆ—é•¿åº¦: {seqlens}")
    print(f"æ€»Tokenæ•°: {total_seqlen}")
    print(f"æœ€å¤§åºåˆ—é•¿åº¦: {max_seqlen}")
    print(f"æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
    print(f"å¤´ç»´åº¦: {head_dim}")
    print(f"MoBA chunk size: {moba_chunk_size}")
    print(f"MoBA top-k: {moba_topk}")
    print("-" * 20)
    # --- æ•°æ®å‡†å¤‡ ---
    # 1. åˆ›å»º Q, K, V å¼ é‡
    # å½¢çŠ¶ä¸º [total_seqlen, num_heads, head_dim]
    q = torch.randn(total_seqlen, num_heads, head_dim, device=device, dtype=dtype, requires_grad=True)
    k = torch.randn(total_seqlen, num_heads, head_dim, device=device, dtype=dtype, requires_grad=True)
    v = torch.randn(total_seqlen, num_heads, head_dim, device=device, dtype=dtype, requires_grad=True)
    # 2. åˆ›å»ºç´¯ç§¯åºåˆ—é•¿åº¦å¼ é‡ (cu_seqlens)
    # è¿™æ˜¯ FlashAttention çš„æ ‡å‡†æ ¼å¼
    cu_seqlens = torch.tensor([0] + seqlens, device=device, dtype=torch.int32).cumsum(dim=0)
    print("--- è¾“å…¥å¼ é‡ä¿¡æ¯ ---")
    print(f"Q å½¢çŠ¶: {q.shape}")
    print(f"K å½¢çŠ¶: {k.shape}")
    print(f"V å½¢çŠ¶: {v.shape}")
    print(f"cu_seqlens: {cu_seqlens}")
    print("-" * 20)
    # --- è¿è¡Œ MoBA æ³¨æ„åŠ› ---
    print(">>> å¼€å§‹æ‰§è¡Œ moba_attn_varlen å‡½æ•°...")
    try:
        output = moba_attn_varlen(
            q=q,
            k=k,
            v=v,
            cu_seqlens=cu_seqlens,
            max_seqlen=max_seqlen,
            moba_chunk_size=moba_chunk_size,
            moba_topk=moba_topk,
        )
        print(">>> moba_attn_varlen å‡½æ•°æ‰§è¡Œå®Œæ¯•ï¼")
        print("-" * 20)
        # --- ç»“æœéªŒè¯ ---
        print("--- è¾“å‡ºç»“æœéªŒè¯ ---")
        print(f"è¾“å‡ºå¼ é‡å½¢çŠ¶: {output.shape}")
        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶æ˜¯å¦æ­£ç¡®
        expected_shape = (total_seqlen, num_heads, head_dim)
        assert output.shape == expected_shape, f"è¾“å‡ºå½¢çŠ¶é”™è¯¯ï¼æœŸæœ›: {expected_shape}, å¾—åˆ°: {output.shape}"
        print("âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®ï¼")
        # æ£€æŸ¥è¾“å‡ºä¸­æ˜¯å¦åŒ…å« NaN æˆ– Inf
        assert not torch.isnan(output).any(), "âŒ è¾“å‡ºä¸­åŒ…å« NaN å€¼ï¼"
        assert not torch.isinf(output).any(), "âŒ è¾“å‡ºä¸­åŒ…å« Inf å€¼ï¼"
        print("âœ… è¾“å‡ºä¸åŒ…å« NaN æˆ– Inf å€¼ã€‚")
        # --- åå‘ä¼ æ’­æµ‹è¯• ---
        print("\n--- åå‘ä¼ æ’­æµ‹è¯• ---")
        # åˆ›å»ºä¸€ä¸ªä¸è¾“å‡ºå½¢çŠ¶ç›¸åŒçš„æ¢¯åº¦å¼ é‡
        grad_output = torch.randn_like(output)
        # æ‰§è¡Œåå‘ä¼ æ’­
        print(">>> å¼€å§‹æ‰§è¡Œ backward()...")
        output.backward(gradient=grad_output)
        print(">>> backward() æ‰§è¡Œå®Œæ¯•ï¼")
        # æ£€æŸ¥ Q, K, V çš„æ¢¯åº¦æ˜¯å¦å­˜åœ¨
        assert q.grad is not None, "âŒ Q çš„æ¢¯åº¦ (q.grad) ä¸å­˜åœ¨ï¼"
        assert k.grad is not None, "âŒ K çš„æ¢¯åº¦ (k.grad) ä¸å­˜åœ¨ï¼"
        assert v.grad is not None, "âŒ V çš„æ¢¯åº¦ (v.grad) ä¸å­˜åœ¨ï¼"
        print("âœ… Q, K, V çš„æ¢¯åº¦å·²æˆåŠŸè®¡ç®—ã€‚")
        # æ£€æŸ¥æ¢¯åº¦å½¢çŠ¶æ˜¯å¦æ­£ç¡®
        assert q.grad.shape == q.shape, f"Q çš„æ¢¯åº¦å½¢çŠ¶é”™è¯¯ï¼æœŸæœ›: {q.shape}, å¾—åˆ°: {q.grad.shape}"
        assert k.grad.shape == k.shape, f"K çš„æ¢¯åº¦å½¢çŠ¶é”™è¯¯ï¼æœŸæœ›: {k.shape}, å¾—åˆ°: {k.grad.shape}"
        assert v.grad.shape == v.shape, f"V çš„æ¢¯åº¦å½¢çŠ¶é”™è¯¯ï¼æœŸæœ›: {v.shape}, å¾—åˆ°: {v.grad.shape}"
        print("âœ… æ¢¯åº¦å½¢çŠ¶æ­£ç¡®ï¼")
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é¡¹å‡å·²é€šè¿‡ï¼")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()